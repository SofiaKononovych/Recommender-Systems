{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THQFNe3zdt1f"
      },
      "source": [
        "- Explore a different recommendation dataset\n",
        " - Develop and evaluate baseline recommender systems\n",
        " - Implement hybrid recommender models\n",
        " - Explore diversification issues in recommender systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvTdMyXdODex"
      },
      "source": [
        "# Part-Pre. Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ww--_kl9-ndn"
      },
      "source": [
        "## Pre 1. Setup Block\n",
        "\n",
        "This exercise will use the [Goodreads]() dataset for books. These blocks setup the data files, Python etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFgYpbhh0tkX",
        "outputId": "1b642532-2d74-4182-e1fe-f84f8292d2f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 7631k  100 7631k    0     0  3738k      0  0:00:02  0:00:02 --:--:-- 3740k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2366k  100 2366k    0     0   632k      0  0:00:03  0:00:03 --:--:--  632k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 7581k  100 7581k    0     0   963k      0  0:00:07  0:00:07 --:--:-- 1221k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1895k  100 1895k    0     0   727k      0  0:00:02  0:00:02 --:--:--  727k\n"
          ]
        }
      ],
      "source": [
        "!rm -rf ratings* books* to_read* test*\n",
        "\n",
        "!curl -o ratings.csv \"https://www.dcs.gla.ac.uk/~craigm/recsysH/coursework/final-ratings.csv\"\n",
        "!curl -o books.csv \"https://www.dcs.gla.ac.uk/~craigm/recsysH/coursework/final-books.csv\"\n",
        "!curl -o to_read.csv \"https://www.dcs.gla.ac.uk/~craigm/recsysH/coursework/final-to_read.csv\"\n",
        "!curl -o test.csv \"https://www.dcs.gla.ac.uk/~craigm/recsysH/coursework/final-test.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VpVnNrZ1EiX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59b3613f-f809-4efc-b974-f8fba60838c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spotlight\n",
            "  Cloning https://github.com/cmacdonald/spotlight.git (to revision seed) to /tmp/pip-install-n52id6uu/spotlight_9838385206bc43be828a892f7633b1bb\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/cmacdonald/spotlight.git /tmp/pip-install-n52id6uu/spotlight_9838385206bc43be828a892f7633b1bb\n",
            "  Running command git checkout -b seed --track origin/seed\n",
            "  Switched to a new branch 'seed'\n",
            "  Branch 'seed' set up to track remote branch 'seed' from 'origin'.\n",
            "  Resolved https://github.com/cmacdonald/spotlight.git to commit 5ae5c189a964b657e913b075ff18f38d8d567c65\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spotlight) (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->spotlight) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->spotlight) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->spotlight) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->spotlight) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->spotlight) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->spotlight) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=0.4.0->spotlight)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=0.4.0->spotlight)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=0.4.0->spotlight)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=0.4.0->spotlight)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=0.4.0->spotlight)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=0.4.0->spotlight)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=0.4.0->spotlight)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=0.4.0->spotlight)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=0.4.0->spotlight)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=0.4.0->spotlight)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=0.4.0->spotlight)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->spotlight) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=0.4.0->spotlight)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->spotlight) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.0->spotlight) (1.3.0)\n",
            "Building wheels for collected packages: spotlight\n",
            "  Building wheel for spotlight (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spotlight: filename=spotlight-0.1.6-py3-none-any.whl size=34068 sha256=04569b8dc4d45c6f66ea158df8342d450932de14df537babf331aa7287e6a1e3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pbu7r5cp/wheels/82/5b/a4/f5c9c5b460c0559524442aa69877d10220d39def2bc62ae6ea\n",
            "Successfully built spotlight\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, spotlight\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 spotlight-0.1.6\n"
          ]
        }
      ],
      "source": [
        "#Standard setup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "!pip install git+https://github.com/cmacdonald/spotlight.git@seed#egg=spotlight\n",
        "from spotlight.interactions import Interactions\n",
        "SEED=20\n",
        "BPRMF=None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtJO0e0m-hun"
      },
      "source": [
        "## Pre 2. Data Preparation\n",
        "\n",
        "Let's load the `goodbooks` dataset into dataframes.\n",
        "- `ratings.csv`: It contains ratings sorted by time. Ratings go from one to five.\n",
        "- `to_read.csv`: It provides IDs of the books marked \"to read\" by each user, as <user_id, book_id> pairs.\n",
        "- `books.csv`: It has metadata for each book (goodreads IDs, authors, title, average rating, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKAb25iw1MYw"
      },
      "outputs": [],
      "source": [
        "#load in the csv files\n",
        "ratings_df = pd.read_csv(\"ratings.csv\")\n",
        "books_df = pd.read_csv(\"books.csv\")\n",
        "to_read_df = pd.read_csv(\"to_read.csv\")\n",
        "test = pd.read_csv(\"test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-mJBqjjEajC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "741d597b-50b3-46f7-e7dc-23baeac69ef6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0  user_id  book_id\n",
              "0      560054     2278     4232\n",
              "1      277900     2118      298\n",
              "2       87083     2769     3934\n",
              "3       77727     2540      293\n",
              "4      240676     3142      147"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4ebaa31c-620d-4fb4-b4d1-0b6feb0cd212\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>user_id</th>\n",
              "      <th>book_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>560054</td>\n",
              "      <td>2278</td>\n",
              "      <td>4232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>277900</td>\n",
              "      <td>2118</td>\n",
              "      <td>298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>87083</td>\n",
              "      <td>2769</td>\n",
              "      <td>3934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>77727</td>\n",
              "      <td>2540</td>\n",
              "      <td>293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>240676</td>\n",
              "      <td>3142</td>\n",
              "      <td>147</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4ebaa31c-620d-4fb4-b4d1-0b6feb0cd212')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4ebaa31c-620d-4fb4-b4d1-0b6feb0cd212 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4ebaa31c-620d-4fb4-b4d1-0b6feb0cd212');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-24b2425b-bc90-41e8-84ef-5568459374c5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-24b2425b-bc90-41e8-84ef-5568459374c5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-24b2425b-bc90-41e8-84ef-5568459374c5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "to_read_df"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "## Test\n",
        "to_read_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6rqfn53OhDC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3120cea4-b19c-40b9-bffc-29f1f6d5d2b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method NDFrame.head of       book_id\n",
            "0           1\n",
            "1           2\n",
            "2           3\n",
            "3           4\n",
            "4           5\n",
            "...       ...\n",
            "1873     1990\n",
            "1874     1991\n",
            "1876     1993\n",
            "1877     1997\n",
            "1879     1999\n",
            "\n",
            "[1826 rows x 1 columns]>\n"
          ]
        }
      ],
      "source": [
        "#cut down the number of items and users\n",
        "counts=ratings_df[ratings_df[\"book_id\"] < 2000].groupby([\"book_id\"]).count().reset_index()\n",
        "valid_books=counts[counts[\"user_id\"] >= 10][[\"book_id\"]]\n",
        "print(valid_books.head)\n",
        "\n",
        "books_df = books_df.merge(valid_books, on=\"book_id\")\n",
        "ratings_df = ratings_df[ratings_df[\"user_id\"] < 2000].merge(valid_books, on=\"book_id\")\n",
        "to_read_df = to_read_df[to_read_df[\"user_id\"] < 2000].merge(valid_books, on=\"book_id\")\n",
        "test = test[test[\"user_id\"] < 2000].merge(valid_books, on=\"book_id\")\n",
        "\n",
        "\n",
        "#stringify the id columns\n",
        "def str_col(df):\n",
        "  if \"user_id\" in df.columns:\n",
        "    df[\"user_id\"] = \"u\" + df.user_id.astype(str)\n",
        "  if \"book_id\" in df.columns:\n",
        "    df[\"book_id\"] = \"b\" + df.book_id.astype(str)\n",
        "\n",
        "str_col(books_df)\n",
        "str_col(ratings_df)\n",
        "str_col(to_read_df)\n",
        "str_col(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7cgXhmYUXIn"
      },
      "source": [
        "Here we construct the Interactions objects from `ratings.csv`, `to_read.csv` and `test.csv`. We manually specify the num_users and num_items parameters to all Interactions objects, in case the test set differs from your training sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15ClgJOdTTt1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf243408-e731-40c9-aafc-cbf1a6eac061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Interactions dataset (1999 users x 1826 items x 124762 interactions)>\n",
            "<Interactions dataset (1999 users x 1826 items x 135615 interactions)>\n",
            "<Interactions dataset (1999 users x 1826 items x 33917 interactions)>\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "from itertools import count\n",
        "\n",
        "from spotlight.cross_validation import random_train_test_split\n",
        "\n",
        "iid_map = defaultdict(count().__next__)\n",
        "\n",
        "\n",
        "rating_iids = np.array([iid_map[iid] for iid in ratings_df[\"book_id\"].values], dtype = np.int32)\n",
        "test_iids = np.array([iid_map[iid] for iid in test[\"book_id\"].values], dtype = np.int32)\n",
        "toread_iids = np.array([iid_map[iid] for iid in to_read_df[\"book_id\"].values], dtype = np.int32)\n",
        "\n",
        "\n",
        "uid_map = defaultdict(count().__next__)\n",
        "test_uids = np.array([uid_map[uid] for uid in test[\"user_id\"].values], dtype = np.int32)\n",
        "rating_uids = np.array([uid_map[uid] for uid in ratings_df[\"user_id\"].values], dtype = np.int32)\n",
        "toread_uids = np.array([uid_map[iid] for iid in to_read_df[\"user_id\"].values], dtype = np.int32)\n",
        "\n",
        "\n",
        "uid_rev_map = {v: k for k, v in uid_map.items()}\n",
        "iid_rev_map = {v: k for k, v in iid_map.items()}\n",
        "\n",
        "\n",
        "rating_dataset = Interactions(user_ids=rating_uids,\n",
        "                               item_ids=rating_iids,\n",
        "                               ratings=ratings_df[\"rating\"].values,\n",
        "                               num_users=len(uid_rev_map),\n",
        "                               num_items=len(iid_rev_map))\n",
        "\n",
        "toread_dataset = Interactions(user_ids=toread_uids,\n",
        "                               item_ids=toread_iids,\n",
        "                               num_users=len(uid_rev_map),\n",
        "                               num_items=len(iid_rev_map))\n",
        "\n",
        "test_dataset = Interactions(user_ids=test_uids,\n",
        "                               item_ids=test_iids,\n",
        "                               num_users=len(uid_rev_map),\n",
        "                               num_items=len(iid_rev_map))\n",
        "\n",
        "print(rating_dataset)\n",
        "print(toread_dataset)\n",
        "print(test_dataset)\n",
        "\n",
        "#here we define the validation set\n",
        "toread_dataset_train, validation = random_train_test_split(toread_dataset, random_state=np.random.RandomState(SEED))\n",
        "\n",
        "num_items = test_dataset.num_items\n",
        "num_users = test_dataset.num_users"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2noK30pBEsF"
      },
      "source": [
        "Finally, this is some utility code that we will use in the exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kDxZgICBFp6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6af31956-1b77-412e-e8a2-259e938aaa90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iid 0: Carlos Ruiz Zafón, Lucia Graves / The Shadow of the Wind (The Cemetery of Forgotten Books,  #1)\n"
          ]
        }
      ],
      "source": [
        "def getAuthorTitle(iid):\n",
        "  bookid = iid_rev_map[iid]\n",
        "  row = books_df[books_df.book_id == bookid]\n",
        "  return row.iloc[0][\"authors\"] + \" / \" + row.iloc[0][\"title\"]\n",
        "\n",
        "print(\"iid 0: \" + getAuthorTitle(0) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt4I2C5DTUL5"
      },
      "source": [
        "## Pre 3. Example Code\n",
        "\n",
        "\n",
        "Here is an example recommender object that returns 0 for each item, regardless of user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2eaxy_hakbC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b87abe9f-ab5d-4dea-8db2-7fe1eefd8ea9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        }
      ],
      "source": [
        "from spotlight.evaluation import mrr_score, precision_recall_score\n",
        "\n",
        "class dummymodel:\n",
        "\n",
        "  def __init__(self, numitems):\n",
        "    self.predictions=np.zeros(numitems)\n",
        "\n",
        "  #uid is the user we are requesting recommendations for;\n",
        "  #returns an array of scores, one for each item\n",
        "  def predict(self, uid):\n",
        "    #this model returns all zeros, regardless of userid\n",
        "    return( self.predictions )\n",
        "\n",
        "#lets evaluate how the effeciveness of dummymodel\n",
        "\n",
        "print(mrr_score(dummymodel(num_items), test_dataset, train=rating_dataset, k=100).mean())\n",
        "#as expected, a recommendation model that gives 0 scores for all items obtains a MRR score of 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQTJOmS5dB3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bc06807-118a-4f69-e848-a7cc4eb384b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1999it [00:00, 2488.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#note that mrr_score() displays a progress bar if you set verbose=True\n",
        "print(mrr_score(dummymodel(num_items), test_dataset, train=rating_dataset, k=100, verbose=True).mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCWXwVC5Mtyj"
      },
      "source": [
        "# Part-A. Combination of Recommendation Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyvGgW_3ZjLV"
      },
      "source": [
        "## Explicit & Implicit Matrix Factorisation Models\n",
        "\n",
        "Create and train three matrix factorisation systems:\n",
        "\n",
        "(NOTE: Different models will be trained using DIFFERENT datasets)\n",
        " - \"EMF\": explicit MF, trained on the **ratings** Interactions object (`rating_dataset`)\n",
        " - \"IMF\": implicit MF, trained on the **toread** Interactions object (`toread_dataset_train`)\n",
        " - \"BPRMF\": implicit MF with the BPR loss function (`loss='bpr'`), trained on the **toread** Interactions object (`toread_dataset_train`)\n",
        "\n",
        "Normally, the hyper-parameters (e.g. `embedding_dim`) will be tuned using the `validation` set based on different models, but here, to simplify the excercie, we use a fixed setting of those hyper-parameters, and keep a fixed random seed.\n",
        "  \n",
        "In all cases, use the standard initialisation arguments, i.e.\n",
        "`n_iter=10, embedding_dim=32, use_cuda=False, random_state=np.random.RandomState(SEED)`.\n",
        "\n",
        "Evaluate each of these models in terms of Mean Reciprocal Rank on the test set. MRR can be obtained using:\n",
        "```python\n",
        "mrr_score(X, test_dataset, train=rating_dataset, k=100, verbose=True).mean())\n",
        "```\n",
        "where X is an instance of a Spotlight model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WExWc-Y9tiBI"
      },
      "source": [
        "### Implement the explicit MF model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySwuwnpDtgDY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c82cd87-769c-49ec-b98b-327e4abbfdfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: loss 3.8710269004595084\n",
            "Epoch 1: loss 0.7940810405817188\n",
            "Epoch 2: loss 0.638251251617416\n",
            "Epoch 3: loss 0.5217335396980654\n",
            "Epoch 4: loss 0.44844858281192235\n",
            "Epoch 5: loss 0.4054335441257133\n",
            "Epoch 6: loss 0.38238635689753003\n",
            "Epoch 7: loss 0.3633662538572413\n",
            "Epoch 8: loss 0.3513794243641076\n",
            "Epoch 9: loss 0.3396693015257355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1999it [00:02, 892.38it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.059"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from spotlight.factorization.explicit import ExplicitFactorizationModel\n",
        "from spotlight.factorization.implicit import ImplicitFactorizationModel\n",
        "from spotlight.losses import bpr_loss\n",
        "\n",
        "EMF = ExplicitFactorizationModel(n_iter=10,\n",
        "                                 embedding_dim=32,\n",
        "                                 use_cuda=False,\n",
        "                                 random_state=np.random.RandomState(SEED))\n",
        "EMF.fit(rating_dataset, verbose=True)\n",
        "\n",
        "x  =  mrr_score(EMF, test_dataset, train=rating_dataset, k=100, verbose=True).mean()\n",
        "x = round(x, 4)\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGi8plPUttEp"
      },
      "source": [
        "### Implement the implicit MF model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUKU7qzctegb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "642c6f46-45c1-4e63-f613-553f10641748"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: loss 0.7677980376020918\n",
            "Epoch 1: loss 0.5387786055370322\n",
            "Epoch 2: loss 0.4701719809112684\n",
            "Epoch 3: loss 0.42832199997215903\n",
            "Epoch 4: loss 0.3983901780590696\n",
            "Epoch 5: loss 0.36827549528119696\n",
            "Epoch 6: loss 0.34734796809981455\n",
            "Epoch 7: loss 0.32980164122890754\n",
            "Epoch 8: loss 0.3187009900597469\n",
            "Epoch 9: loss 0.3048194520316034\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1999it [00:02, 873.49it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3035"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Add your solution here\n",
        "IMF = ImplicitFactorizationModel(n_iter=10,\n",
        "                                 embedding_dim=32,\n",
        "                                 use_cuda=False,\n",
        "                                 random_state=np.random.RandomState(SEED))\n",
        "IMF.fit(toread_dataset_train, verbose=True)\n",
        "IMF_score = mrr_score(IMF, test_dataset, train=toread_dataset_train, k=100, verbose=True).mean()\n",
        "x = round(IMF_score, 4)\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sETn-utit1TN"
      },
      "source": [
        "### Implement the BPRMF model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDADjtepRvpJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41906b12-7bd1-44ac-989a-93ba69c71281"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: loss 0.3389544660612097\n",
            "Epoch 1: loss 0.1964499857276678\n",
            "Epoch 2: loss 0.15870639708174286\n",
            "Epoch 3: loss 0.14147727969893306\n",
            "Epoch 4: loss 0.1328272745891843\n",
            "Epoch 5: loss 0.12213623430579901\n",
            "Epoch 6: loss 0.11668405982331848\n",
            "Epoch 7: loss 0.11047121703202994\n",
            "Epoch 8: loss 0.1088867612745402\n",
            "Epoch 9: loss 0.1040047079693737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1999it [00:02, 892.52it/s]\n",
            "1999it [00:02, 879.23it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1999"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "BPRMF = ImplicitFactorizationModel(loss='bpr', n_iter=10,\n",
        "                                 embedding_dim=32,\n",
        "                                 use_cuda=False,\n",
        "                                 random_state=np.random.RandomState(SEED))\n",
        "BPRMF.fit(toread_dataset_train, verbose=True)\n",
        "\n",
        "BPRMF_score = mrr_score(BPRMF, test_dataset, train=rating_dataset, k=100, verbose=True).mean()\n",
        "x = round(BPRMF_score, 4)\n",
        "x\n",
        "\n",
        "BPRMF_scores = mrr_score(BPRMF, test_dataset, train=rating_dataset, k=100, verbose=True)\n",
        "len(BPRMF_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSD9ZqakuLxv"
      },
      "source": [
        "Now you can answer quiz question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZHCOmfEDOGo"
      },
      "source": [
        "## Hybrid Model\n",
        "\n",
        "(a) Linearly combine the *scores* from IMF and BPRMF.  \n",
        "(b) Apply a pipelining recommender, where the top 100 items are obtained from IMF and re-ranked using the scores of BPRMF. Items not returned by IMF get a score of 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6j6JzIOHkYw9"
      },
      "outputs": [],
      "source": [
        "def test_Hybrid_a(combsumObj):\n",
        "  for i, u in enumerate([5, 20]):\n",
        "    print(\"Hybrid a test case %d\" % i)\n",
        "    print(np.count_nonzero(combsumObj.predict(u) > 1))\n",
        "\n",
        "def test_Hybrid_b(pipeObj):\n",
        "  for i, iid in enumerate([3, 0]):\n",
        "    print(\"Hybrid b test case %d\" % i)\n",
        "    print(pipeObj.predict(0)[iid])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_o7a1ppFZ7R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0c69702-960f-42fd-c0bc-c7cb578983b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1999it [00:06, 307.76it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "736"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "#from sklearn.preprocessing import minmax_scale\n",
        "\n",
        "#Implement combsum hybrid model\n",
        "class CombSumModel:\n",
        "    def __init__(self, IMF, BPRMF):\n",
        "        self.IMF = IMF\n",
        "        self.BPRMF = BPRMF\n",
        "\n",
        "    def predict(self, uid):\n",
        "        # Normalise obtained predicted scores\n",
        "        scores_imf_norm = minmax_scale(self.IMF.predict(uid))\n",
        "        scores_bprmf_norm = minmax_scale(self.BPRMF.predict(uid))\n",
        "\n",
        "        # Combine scores by summing them\n",
        "        combined_scores = scores_imf_norm + scores_bprmf_norm\n",
        "\n",
        "        return combined_scores\n",
        "\n",
        "linearModel = CombSumModel(IMF, BPRMF)\n",
        "\n",
        "combsum_scores = mrr_score(linearModel, test_dataset, train=rating_dataset, k=100, verbose=True)\n",
        "combsum_score = combsum_scores.mean()\n",
        "\n",
        "improved_scores_count = sum(a > b for a, b in zip(combsum_scores, BPRMF_scores))\n",
        "degraded_scores_count = sum(a < b for a, b in zip(combsum_scores, BPRMF_scores))\n",
        "improved_scores_count"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement pipeline hybrid model\n",
        "class PipelineModel:\n",
        "    def __init__(self, IMF, BPRMF):\n",
        "        self.IMF = IMF\n",
        "        self.BPRMF = BPRMF\n",
        "\n",
        "    def predict(self, uid):\n",
        "        # Obtain top 100 scores predicted from IMF\n",
        "        top100_imf = self.IMF.predict(uid).argsort()[:100]\n",
        "\n",
        "# Normalise scores predicted from BPRMF and get corresponding to top 100 IMF scores\n",
        "        bprmf_scores = minmax_scale(self.BPRMF.predict(uid))\n",
        "        bprmf_scores = bprmf_scores[top100_imf]\n",
        "        # Items not returned by IMF get a score of 0\n",
        "        reranked_scores = np.array([bprmf_scores[i] if i < len(bprmf_scores) else 0 for i in range(self.IMF._num_items)])\n",
        "        return reranked_scores\n",
        "\n",
        "pipeModel = PipelineModel(IMF, BPRMF)\n",
        "\n",
        "pipeline_scores = mrr_score(pipeModel, test_dataset, train=rating_dataset, k=100, verbose=True)\n",
        "pipeline_score = pipeline_scores.mean()\n",
        "improved_scores_count = sum(a > b for a, b in zip(pipeline_scores, BPRMF_scores))\n",
        "degraded_scores_count = sum(a < b for a, b in zip(pipeline_scores, BPRMF_scores))\n",
        "improved_scores_count, degraded_scores_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFHfppF2rCjN",
        "outputId": "fd27549e-6be2-4b61-89de-2301893d9da9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1999it [00:05, 365.84it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(272, 1653)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROCBxlfKKkGA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "963283e8-5d4d-4298-b6a4-e6354c53c17a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hybrid a test case 0\n",
            "445\n",
            "Hybrid a test case 1\n",
            "407\n",
            "Hybrid b test case 0\n",
            "0.12721455097198486\n",
            "Hybrid b test case 1\n",
            "0.1450352966785431\n"
          ]
        }
      ],
      "source": [
        "#Now test hybrid approaches\n",
        "\n",
        "test_Hybrid_a(linearModel)\n",
        "test_Hybrid_b(pipeModel)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf0K6GECM0LQ"
      },
      "source": [
        "# Part-B. Analysing Recommendation Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gszqk2kIZLX"
      },
      "source": [
        "## Utility methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIyc_p_dIm1M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ae8df4b-53ff-4fbf-a6fd-75b6a82f7445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 204.  596.  922. ... 1753. 1142. 1742.]\n",
            "Returned iids: [ 23 108  21  33   9  81  52 254  16   3]\n",
            "Returned scores: [0.99999994 0.9895164  0.98483366 0.9225092  0.9070964  0.9065484\n",
            " 0.9005373  0.8931043  0.8837875  0.88369954]\n",
            "Returned embeddings: tensor([[-0.0454,  1.3716, -0.8307, -1.2616,  1.6699,  1.0161,  1.1168,  2.3530,\n",
            "         -1.2027,  0.8522, -1.0941, -0.6864, -0.5725, -2.0335, -1.2591,  0.6154,\n",
            "         -0.1374, -1.6868, -1.8616, -0.7514,  1.9909, -0.3909,  1.9239,  1.3293,\n",
            "         -1.2834, -0.4520,  1.1338,  0.3468,  2.5168, -2.1586,  1.2310,  1.1670],\n",
            "        [ 0.1239,  1.1003,  0.0531, -1.1045,  1.9932,  1.5049,  1.0011,  1.9734,\n",
            "         -1.6322, -0.8913, -0.6372,  0.7721, -1.1422, -2.2424, -1.1936, -0.5770,\n",
            "          0.0762, -1.0283, -1.2806, -2.0889,  2.8154, -0.9600, -0.1419,  0.8408,\n",
            "         -1.6067, -1.2905,  1.9168,  1.3988,  1.8646, -2.2029,  0.5365,  0.2022],\n",
            "        [ 0.3844,  0.8189, -0.1892, -1.1793,  2.1731,  0.6669,  1.1271,  1.4538,\n",
            "         -1.2173, -0.5447, -1.6714,  0.5249, -0.6132, -3.1082, -0.6489,  0.4313,\n",
            "          0.9176, -1.0347, -1.7232, -1.3347,  2.5504,  0.2789,  1.9649,  0.7684,\n",
            "         -1.0310, -1.3983,  0.8985, -0.0561,  2.1894, -0.8905,  1.0992,  0.6691],\n",
            "        [-0.4411,  0.4801, -0.2539, -0.5986,  1.2272,  0.6531,  1.4534,  1.3802,\n",
            "         -1.3797,  0.8306, -1.1837, -0.3367, -0.3527, -1.9982, -1.2018,  0.8934,\n",
            "         -0.5632, -0.6443, -0.7337, -0.4921,  2.9899,  0.2761,  1.4479,  1.0105,\n",
            "         -0.7107, -1.7104, -0.9456, -0.2314,  2.2862, -1.0982,  0.6176,  1.9784],\n",
            "        [-0.5686,  1.3279,  0.0929, -1.1565,  0.5139, -0.1223,  0.8788,  2.0444,\n",
            "          0.2802,  0.6416, -0.3809,  0.2828, -0.3895, -2.7014, -1.4182,  0.2743,\n",
            "         -1.0461, -1.5824, -2.0993, -1.3979,  1.3412, -0.4345,  1.5427,  1.2284,\n",
            "         -2.0169, -1.3084,  0.2939,  2.2792,  1.2570, -0.8994,  1.0785, -0.0203],\n",
            "        [ 0.1340,  0.2584, -0.5792, -0.5029,  2.7529,  0.0107,  0.8058,  2.3262,\n",
            "         -1.9018, -0.4165, -1.4422, -0.7689, -0.7657, -1.3836,  0.7729, -0.0596,\n",
            "          0.1377, -0.9144, -1.0304, -2.4873,  2.4421, -0.2146,  1.3113,  2.0114,\n",
            "         -0.5655, -1.5422,  1.9430,  2.1211,  1.2265, -0.4562,  0.4370,  1.1741],\n",
            "        [ 0.5934,  1.3073,  0.5726, -0.0917,  1.6623,  1.3125,  0.8131,  1.4753,\n",
            "         -1.6077,  1.4744, -0.6149, -0.1319,  0.2843, -2.1551, -1.0225,  1.1612,\n",
            "         -0.7732, -1.3496, -0.7587, -1.4566,  1.8771,  0.2448,  0.9533,  0.2902,\n",
            "         -1.4033, -1.9129,  1.0374, -0.1573,  2.0627, -1.1652,  0.8939,  0.7757],\n",
            "        [ 0.6018,  1.0445, -0.5415,  0.5355,  1.4569,  0.5330,  0.2956,  1.5574,\n",
            "         -0.2669, -1.4242,  1.5775,  1.0870, -0.6438, -1.5679, -1.4657,  1.3033,\n",
            "         -0.6602, -0.7102, -1.1307, -1.5142,  1.2747,  0.5494, -0.2278,  1.8629,\n",
            "         -1.8720, -0.3860,  1.0929,  1.4838,  1.2602, -1.6315, -0.4450,  0.6793],\n",
            "        [-0.4931, -0.2156, -1.0300, -1.1251,  2.3141,  0.1844,  0.0278,  1.1525,\n",
            "         -0.3218, -0.2236, -0.9952,  0.4091, -0.8534, -2.1377, -2.0955,  0.4107,\n",
            "         -0.5804, -1.6455, -1.4729, -2.6273,  1.5917, -0.3359,  2.3430,  0.6596,\n",
            "         -1.4888, -1.8436,  1.2947,  2.4997,  1.9382, -0.2631,  0.8980,  0.6717],\n",
            "        [-0.6251,  1.0291, -0.9706, -0.5551,  1.3933,  1.4241,  0.6316,  0.8137,\n",
            "         -0.1443, -0.4631,  0.1315, -0.3589, -0.3534, -1.7653, -0.1728,  0.4081,\n",
            "         -2.4594, -2.0278, -0.9450, -1.9469,  0.8574, -0.0175,  0.7410,  0.8269,\n",
            "         -0.9872, -1.0237,  1.6763,  1.2756,  1.2098, -0.8075,  1.2227,  1.8007]])\n"
          ]
        }
      ],
      "source": [
        "from typing import Sequence, Tuple\n",
        "\n",
        "def get_top_K(model, uid : int, k : int) -> Tuple[ Sequence[int], Sequence[float],  np.ndarray ] :\n",
        "  #returns iids, their (normalised) scores in descending order, and item emebddings for the top k predictions of the given uid.\n",
        "\n",
        "  from sklearn.preprocessing import minmax_scale\n",
        "\n",
        "  from scipy.stats import rankdata\n",
        "  # get scores from model\n",
        "  scores = model.predict(uid)\n",
        "\n",
        "  # map scores into rank 0..1 over the entire item space\n",
        "  scores = minmax_scale(scores)\n",
        "\n",
        "  #compute their ranks\n",
        "  ranks = rankdata(-scores)\n",
        "  print(ranks)\n",
        "\n",
        "  # get and filter iids, scores and embeddings\n",
        "  rtr_scores = scores[ranks <= k]\n",
        "  rtr_iids = np.argwhere(ranks <= k).flatten()\n",
        "  if hasattr(model, '_net'):\n",
        "    embs = model._net.item_embeddings.weight[rtr_iids].detach()\n",
        "  else:\n",
        "    # not a model that has any embeddings\n",
        "    embs = np.zeros([k,1])\n",
        "\n",
        "  # identify correct ordering using numpy.argsort()\n",
        "  ordering = (-1*rtr_scores).argsort()\n",
        "\n",
        "  #return iids, scores and their embeddings in descending order of score\n",
        "  return rtr_iids[ordering], rtr_scores[ordering], embs[ordering]\n",
        "\n",
        "if BPRMF is not None: # BPRMF is the model name defined in Task 1\n",
        "  iids, scores, embs = get_top_K(BPRMF, 0, 10)\n",
        "  print(\"Returned iids: %s\" % str(iids))\n",
        "  print(\"Returned scores: %s\" % str(scores))\n",
        "  print(\"Returned embeddings: %s\" % str(embs))\n",
        "else:\n",
        "  print(\"You need to define BPRMF in Task 1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZqRSixpGvfn"
      },
      "source": [
        "## Qualiatively Examining Recommendations\n",
        "\n",
        "From now on, we will consider the `BPRMF` model.\n",
        "\n",
        "Write a function, which given a uid (int), prints the *title and authors* of:\n",
        " - (a) the books that the user has previously shelved (c.f. `toread_dataset_train`)\n",
        " - (b) the books that the user will read in the future (c.f. `test_dataset`)\n",
        " - (c) the top 10 books that the user were recommended by `BPRMF` - you can make use of `get_top_K()`.\n",
        "\n",
        "Then, we will examine two specific users, namely uid 1805 (u336) and uid 179 (user u1331), to analyse if their recommendations make sense."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kg1eFa5GYv5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e084ac5-eeb1-4149-967c-b538ca86e815"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 446.  260.  575. ... 1006. 1095. 1404.]\n",
            "['Suzanne Collins / The Hunger Games (The Hunger Games, #1)', 'Dan Brown / The Da Vinci Code (Robert Langdon, #2)', 'Dan Brown / The Lost Symbol (Robert Langdon, #3)', 'Michael Crichton / Disclosure', 'George R.R. Martin / A Clash of Kings  (A Song of Ice and Fire, #2)', 'Dan Brown / Angels & Demons  (Robert Langdon, #1)', 'John Grisham / The Broker', 'Khaled Hosseini / The Kite Runner', 'George R.R. Martin / A Game of Thrones (A Song of Ice and Fire, #1)', 'Suzanne Collins / Mockingjay (The Hunger Games, #3)']\n",
            "['John Grisham / The Pelican Brief', 'Stieg Larsson, Reg Keeland / The Girl Who Played with Fire (Millennium, #2)', 'Gillian Flynn / Gone Girl', 'Tom Clancy / The Hunt for Red October (Jack Ryan Universe, #4)', 'Chuck Palahniuk / Fight Club', 'Umberto Eco, William Weaver, Seán Barrett / The Name of the Rose', 'John Grisham / The Runaway Jury', 'Thomas Harris / Hannibal (Hannibal Lecter, #3)', 'Lee Child / The Affair (Jack Reacher, #16)', 'John Grisham / The Firm (Penguin Readers, Level 5)', 'Lee Child / Killing Floor (Jack Reacher, #1)', 'John Grisham / A Time to Kill', 'Stephen King / The Shining (The Shining #1)', 'Michael Crichton / Timeline', 'Michael Crichton / Prey', 'Jeffery Deaver / The Bone Collector (Lincoln Rhyme, #1)']\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "def get_title_author(uid : int):\n",
        "  user_indices_toread = np.where(toread_dataset_train.user_ids == uid)[0]\n",
        "  user_indices_test = np.where(test_dataset.user_ids == uid)[0]\n",
        "\n",
        "  book_ids_toread = toread_dataset_train.item_ids[user_indices_toread]\n",
        "  book_ids_test = test_dataset.item_ids[user_indices_test]\n",
        "  iids, scores, emb = get_top_K(BPRMF, uid, 10)\n",
        "\n",
        "  def get_author_title_list(book_ids):\n",
        "    res = [getAuthorTitle(id) for id in book_ids]\n",
        "    return res\n",
        "\n",
        "  res_a = get_author_title_list(book_ids_toread)\n",
        "  res_b = get_author_title_list(book_ids_test)\n",
        "  res_c = get_author_title_list(iids)\n",
        "\n",
        "  return res_a, res_b, res_c\n",
        "\n",
        "res_a_1805, res_b_1805, res_c_1805 = get_title_author(1805)\n",
        "\n",
        "score_1805 = BPRMF_scores[1805]\n",
        "\n",
        "prev_shelved_1805 = len([b for b in res_c_1805 if b in res_b_1805])\n",
        "print(res_c_1805)\n",
        "print(res_b_1805)\n",
        "print(prev_shelved_1805)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES_zHeCkNBeC"
      },
      "source": [
        "# Part-C. Diversity of Recommendations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvBep-ROHWSX"
      },
      "source": [
        "## Measuring Intra-List Diversity\n",
        "\n",
        "\n",
        "For the BPR implicit factorisation model, implement the Intra-list diversity measure of the top 5 scored items based on their item embeddings in the `BPRMF` model.\n",
        "\n",
        "Implement your ILD as a function with the specification:\n",
        "```python\n",
        "def measure_ild(top_books : Sequence[int], K : int=5) -> float\n",
        "```\n",
        "where:\n",
        " - `top_books` is a list or a Numpy array of iids that have been returned for a particular user. For instance, it can be obtained from `get_top_K()`.\n",
        " - `K` is the number of top-ranked items to consider from `top_books`.\n",
        " - Your implementation should use the item embeddings stored in the `BPRMF` model.\n",
        "\n",
        "Calculate the ILD (with k=5), identify the books previously shelved and recommended for the specific users requested in the quiz, and use these to analyse the recommendations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4n2vBwcnYuM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "248236c8-3b23-4ed0-fa7e-625d7b4f194c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 446.  260.  575. ... 1006. 1095. 1404.]\n",
            "[ 646.  976.  490. ... 1043. 1525. 1523.]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['John Grisham / The Partner',\n",
              " 'John Grisham / The Pelican Brief',\n",
              " 'John Grisham / The Client',\n",
              " 'John Grisham / The Brethren',\n",
              " 'John Grisham / The Street Lawyer',\n",
              " 'John Grisham / The Broker',\n",
              " 'John Grisham / The Rainmaker',\n",
              " 'John Grisham / The King of Torts',\n",
              " \"J.K. Rowling, Mary GrandPré / Harry Potter and the Sorcerer's Stone (Harry Potter, #1)\",\n",
              " 'John Grisham / The Runaway Jury']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "import torch.nn.functional as f\n",
        "\n",
        "def measure_ild(top_books : Sequence[int], K : int=5) -> float:\n",
        "  # retrieve K embeddings\n",
        "  top_embeddings  = BPRMF._net.item_embeddings.weight[top_books[:K]]\n",
        "  # calculate cosine similarities for top embeddings like matrix entries i, j\n",
        "  ild_sum = 0\n",
        "  for i in range(K):\n",
        "      for j in range(i + 1, K):\n",
        "          similarity = 1 - f.cosine_similarity(top_embeddings[i], top_embeddings[j], axis=0)\n",
        "          ild_sum += similarity.item()\n",
        "  # calculate ILD using formula\n",
        "  ild = (2 / (K * (K - 1))) * ild_sum\n",
        "  return ild\n",
        "\n",
        "top_books_1805 , _, _ = get_top_K(BPRMF, 1805, 5)\n",
        "ild_1805 = measure_ild(top_books_1805, 5)\n",
        "authors_1805 = [getAuthorTitle(iid) for iid in top_books_1805]\n",
        "\n",
        "top_books_179 , _, _ = get_top_K(BPRMF, 179, 10)\n",
        "ild_179 = measure_ild(top_books_179, 5)\n",
        "authors_179 =[getAuthorTitle(iid) for iid in top_books_179]\n",
        "authors_179\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qwrP1jUpARF"
      },
      "source": [
        "## Task 5. Implement MMR Diversification\n",
        "\n",
        "Develop an Maximal Marginal Relevance (MMR) diversification technique, to re-rank the top-ranked recommendations for a given user.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VkEMfRvIhKV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1a22a6f-87b6-496c-ec8f-b29fba0c187c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 204.  596.  922. ... 1753. 1142. 1742.]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[108, 21, 23, 81, 33, 3, 254, 9, 16, 52]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "from typing import Sequence\n",
        "from itertools import combinations\n",
        "def mmr(iids : Sequence[int], scores : Sequence[float], embs : np.ndarray, alpha : float) -> Sequence[int]:\n",
        "\n",
        "  assert len(iids) == len(scores)\n",
        "  assert len(iids) == embs.shape[0]\n",
        "  assert len(embs.size()) == 2\n",
        "\n",
        "  relevance = scores\n",
        "  diversity = [1 - f.cosine_similarity(embs[i], embs[j], dim=0) for i, j in combinations(range(len(iids)), 2)]\n",
        "\n",
        "    # Calculate the MMR score for each item\n",
        "  mmr_scores = [alpha * rel - (1 - alpha) * div for rel, div in zip(relevance, diversity)]\n",
        "\n",
        "    # Sort items in descending order of MMR score\n",
        "  sorted_indices = np.argsort(mmr_scores)[::-1]\n",
        "\n",
        "    # Reorder iids based on the sorting\n",
        "  rtr_iids = [iids[idx] for idx in sorted_indices]\n",
        "  return rtr_iids\n",
        "\n",
        "iids = mmr( *get_top_K(BPRMF, 0, 10), 0.5)\n",
        "iids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34QWxFVTfrLM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daf230e8-381f-4034-b4be-3fc453d2aaeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testcase 0 : 1\n",
            "Testcase 1 : 3\n",
            "Testcase 2 : 2\n",
            "Testcase 3 : 2\n",
            "Testcase 4 : 2\n"
          ]
        }
      ],
      "source": [
        "def run_MMR_testcases(mmrfn):\n",
        "  example_embeddings1 = torch.tensor([[1.0,1.0],[1.0,1.0],[0,1.0],[0.1, 1.0]])\n",
        "  example_embeddings2 = torch.tensor([[1.0,1.0],[1.0,1.0],[0.02,1.0],[0.01,1.0]])\n",
        "  print(\"Testcase 0 : %s\" % mmrfn([1,2,3,4], [0.5, 0.5, 0.5, 0.5],  example_embeddings1, 0.5)[0] )\n",
        "  print(\"Testcase 1 : %s\" % mmrfn([1,2,3,4], [0.5, 0.5, 0.5, 0.5],  example_embeddings1, 0.5)[1] )\n",
        "  print(\"Testcase 2 : %s\" % mmrfn([1,2,3,4], [4, 3, 2, 1],  example_embeddings1, 1)[1] )\n",
        "  print(\"Testcase 3 : %s\" % mmrfn([1,2,3,4], [0.99, 0.98, 0.97, 0.001],  example_embeddings2, 0.001)[1] )\n",
        "  print(\"Testcase 4 : %s\" % mmrfn([1,2,3,4], [0.99, 0.98, 0.97, 0.001],  example_embeddings2, 0.5)[1] )\n",
        "\n",
        "run_MMR_testcases(mmr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfm9mCWZmBPQ"
      },
      "source": [
        "Now we can analyse the impact of our MMR implementation. Let's consider again uid 179 (user u1331).\n",
        "\n",
        "Apply MMR on the top 10 results obtained from the BPRMF model using `get_top_K()`, with an alpha value of 0.5. The following code should help:\n",
        "```python\n",
        "mmr( *get_top_K(BPRMF, 179, 10), 0.5)\n",
        "```\n",
        "\n",
        "Finally, anayse the returned books. Calculate the ILD (with `k=5`), and examine the authors and titles (using `getAuthorTitle()`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wM7m8pOmCnM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4d05a9b-3fca-4452-e8c8-849fb8a372df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 646.  976.  490. ... 1043. 1525. 1523.]\n",
            "['John Grisham / The Partner', 'John Grisham / The Client', 'John Grisham / The Runaway Jury', 'John Grisham / The Pelican Brief', 'John Grisham / The Street Lawyer', 'John Grisham / The Rainmaker', 'John Grisham / The Broker', 'John Grisham / The Brethren', \"J.K. Rowling, Mary GrandPré / Harry Potter and the Sorcerer's Stone (Harry Potter, #1)\", 'John Grisham / The King of Torts']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2928944885730744"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "iids_179 = mmr( *get_top_K(BPRMF, 179, 10), 0.5)\n",
        "items_recc = [getAuthorTitle(iid) for iid in iids_179]\n",
        "print(items_recc)\n",
        "ild_179 = measure_ild(iids_179, 5)\n",
        "ild_179"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}